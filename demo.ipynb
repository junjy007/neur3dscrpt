{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Data Preparation\n",
    "#################\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset = []\n",
    "data_root = './data/tmp'\n",
    "for obj_dir in os.listdir(data_root):\n",
    "    images = []\n",
    "    for i in range(12):\n",
    "        im_path = os.path.join(data_root, obj_dir, f\"a_{i:0>3}_depth0001.png\")\n",
    "        images.append(\n",
    "            np.array(Image.open(im_path).convert('RGB')))\n",
    "    \n",
    "    spt_path = os.path.join(data_root, obj_dir, \"a_script.spt\")\n",
    "    with open(spt_path) as file:\n",
    "        spt = file.read()\n",
    "\n",
    "    dataset.append({\n",
    "        \"images\": images,\n",
    "        \"script\": spt,\n",
    "    })\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),     \n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "     \n",
    "def load_obj_data(idx):\n",
    "    im_pth = []\n",
    "    for im in dataset[idx]['images']:\n",
    "        im_pth.append(transform(im))\n",
    "    im_pth = torch.stack(im_pth, dim=0)\n",
    "    return im_pth, dataset[idx]['script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Model\n",
    "#################\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from transformers import CodeGenTokenizer, CodeGenForCausalLM\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "MODEL_PATH = os.path.expanduser(\"./codegen-350M-mono/\")\n",
    "tokenizer = CodeGenTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = CodeGenForCausalLM.from_pretrained(MODEL_PATH).to(device)\n",
    "\n",
    "class FuseEmb(nn.Module):\n",
    "    def __init__(self, real_emb) -> None:\n",
    "        super().__init__()\n",
    "        self.prompt = True\n",
    "        self.real_emb = real_emb\n",
    "\n",
    "        # [batch, 3, H, W] -> [batch, 1024]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(119072, 1024),\n",
    "        )\n",
    "        self.im_prompt = None\n",
    "\n",
    "    def set_im_input(self, images):\n",
    "        h = self.encoder(images)\n",
    "        self.im_prompt = h.view(1, len(images), 1024)\n",
    "        self.prompt = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.prompt:\n",
    "            # print(\"prompt\", input.shape)\n",
    "            self.prompt = False\n",
    "            e = self.real_emb(input)\n",
    "            N_prompt = self.im_prompt.shape[1]\n",
    "            e[:, :N_prompt, :] = self.im_prompt\n",
    "            return e\n",
    "        else:\n",
    "            return self.real_emb(input)\n",
    "\n",
    "# [key step] replace embedding layer\n",
    "emb = FuseEmb(model.transformer.wte).to(device)\n",
    "model.transformer.wte = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/cenbylin/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/codegen/modeling_codegen.py:167: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/TensorCompare.cpp:402.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n",
      "loss=0.0002: 100%|██████████| 100/100 [02:49<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Fine-tune\n",
    "#################\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "num_epochs = 100\n",
    "num_training_steps = num_epochs * len(dataset)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "# )\n",
    "\n",
    "pbar = tqdm(range(num_epochs))\n",
    "for epoch in pbar:\n",
    "    for idx in range(len(dataset)):\n",
    "        model.train()\n",
    "\n",
    "        ims, spt = load_obj_data(idx)\n",
    "        n_ims = ims.shape[0]\n",
    "        \n",
    "        fake_prompt = torch.arange(0, n_ims).view(1, -1).long()\n",
    "        spt_ids = tokenizer(spt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        full_text = torch.cat([fake_prompt, spt_ids], dim=1).to(device)\n",
    "        attention_mask = torch.tensor([[1]*fake_prompt.shape[1] + [0]*spt_ids.shape[1]]).to(device)\n",
    "\n",
    "        model.transformer.wte.set_im_input(ims.to(device))\n",
    "        outputs = model(full_text, \n",
    "                        labels=full_text, \n",
    "                        attention_mask=attention_mask)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_description(f\"loss={loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Prediction]\n",
      "\n",
      "ADD CUBE\n",
      "SELECT FACE LOCATION LEFT 2\n",
      "RESIZE ALL -\n",
      "DELETE\n",
      "SOLIDIFY 7\n",
      "MOD_\n",
      "SOLIDIFY 3\n",
      "\n",
      "\n",
      "[Ground Truth]\n",
      "\n",
      "ADD CUBE\n",
      "SELECT FACE LOCATION LEFT 2\n",
      "RESIZE ALL - 4\n",
      "DELETE\n",
      "MOD_SOLIDIFY 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Test\n",
    "#################\n",
    "idx = 0\n",
    "ims, spt = load_obj_data(idx)\n",
    "n_ims = ims.shape[0]\n",
    "\n",
    "fake_prompt = torch.arange(0, n_ims).view(1, -1).long().to(device)\n",
    "model.transformer.wte.set_im_input(ims.to(device))\n",
    "generated_ids = model.generate(fake_prompt, max_length=50)\n",
    "\n",
    "print(f\"\\n\\n[Prediction]\\n\")\n",
    "print(tokenizer.decode(generated_ids[0][n_ims:], skip_special_tokens=True))\n",
    "print(f\"\\n\\n[Ground Truth]\\n\")\n",
    "print(spt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a690cb9fee9b883a4cce522b3627075ee130642564ac574a4d8bf476f3fd800f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
